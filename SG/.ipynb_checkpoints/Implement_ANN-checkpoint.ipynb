{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPW3NfqmXbze"
   },
   "source": [
    "# Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e6JOs1iPRFS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Yzf_4OnQU8C"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "8v7SViMPRF7X",
    "outputId": "8c2746b8-a413-49de-e24d-5cfa5fb7d355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (50000, 32, 32, 3)\n",
      "y_train shape :  (50000, 32, 32, 3)\n",
      "X_train shape :  (50000, 32, 32, 3)\n",
      "y_train shape :  (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape : ', X_train.shape)\n",
    "print('y_train shape : ', X_train.shape)\n",
    "print('X_train shape : ', X_train.shape)\n",
    "print('y_train shape : ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO2yIvffRsra"
   },
   "outputs": [],
   "source": [
    "X_val = X_train[-1000:,:]\n",
    "y_val = y_train[-1000:]\n",
    "\n",
    "X_train = X_train[:-1000,:]\n",
    "y_train = y_train[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "5uYBQzuDSauy",
    "outputId": "3f0270d6-e010-46d8-ec42-ae069a66cf8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (49000, 32, 32, 3)\n",
      "y_train shape :  (49000, 32, 32, 3)\n",
      "X_val shape   :  (1000, 32, 32, 3)\n",
      "y_val shape   :  (1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape : ', X_train.shape)\n",
    "print('y_train shape : ', X_train.shape)\n",
    "print('X_val shape   : ', X_val.shape)\n",
    "print('y_val shape   : ', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVS9fMBUSku9"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "7SFICB32UVJm",
    "outputId": "74727e3a-4f51-4efc-e463-235b52e814c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (49000, 3072)\n",
      "X_val.shape :  (1000, 3072)\n",
      "X_test.shape :  (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_val.shape : ', X_val.shape)\n",
    "print('X_test.shape : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "c9VvJzInUVFS",
    "outputId": "1ac69307-6df8-4935-9f1a-37d869c71577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape :  (49000,)\n",
      "y_val.shape   :  (1000,)\n",
      "y_test.shape  :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.ravel()\n",
    "y_val = y_val.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "print('y_train.shape : ',y_train.shape)\n",
    "print('y_val.shape   : ',y_val.shape)\n",
    "print('y_test.shape  : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsBZggO5Xglb"
   },
   "source": [
    "#defining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mex9ziS4XsPL"
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, W, b):\n",
    "  return x.dot(W) + b\n",
    "\n",
    "def affine_backward(dout, x, W, b):\n",
    "  dW = x.T.dot(dout)\n",
    "  db = np.sum(dout, axis = 0, keepdims = True)\n",
    "  dx = dout.dot(W.T)\n",
    "  return dW, db, dx\n",
    "\n",
    "def relu_forward(x):\n",
    "  out = np.maximum(0,x)\n",
    "  return out\n",
    "\n",
    "def relu_backward(dout, x):\n",
    "  dr_prime = np.where((x<0),0,1)\n",
    "  dout = dout*dr_prime\n",
    "  return dout\n",
    "\n",
    "def softmax(x):\n",
    "  x -= np.max(x)\n",
    "  x_exp = np.exp(x)\n",
    "  x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "  score = x_exp/x_sum\n",
    "  return score\n",
    "\n",
    "def softmax_loss(score,y):\n",
    "  num_examples = score.shape[0]\n",
    "  \n",
    "  number_list = range(num_examples)\n",
    "  correct_logprobs = -np.log(score[number_list,y])\n",
    "  loss = np.sum(correct_logprobs)/num_examples\n",
    "  \n",
    "  dscores = score \n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  return loss,dscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoFNEazhbpCq"
   },
   "outputs": [],
   "source": [
    "def predict_two_layer_relu(x, W, b):\n",
    "  y_pred = np.zeros(x.shape[0])\n",
    "  \n",
    "  layer1 = affine_forward(x, W[0], b[0])\n",
    "  act1= relu_forward(layer1)\n",
    "  \n",
    "  layer2 = affine_forward(act1, W[1], b[1])\n",
    "  y_pred = np.argmax(layer2, axis=-1 )\n",
    "  \n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QsB3ti9d0iT"
   },
   "outputs": [],
   "source": [
    "def train_two_layer_relu(X, y, X_val, y_val, hidden_size, W=None, b=None, learning_rate=1e-4, \n",
    "                         lr_decay = 0.9, reg = 0.5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "  num_train, dim = X.shape\n",
    "  iterations_per_epoch = max(num_train/batch_size, 1)\n",
    "  num_classes = np.max(y) +1\n",
    "  \n",
    "  if W is None:\n",
    "    W0 = 1e-4 * np.random.randn(dim, hidden_size)\n",
    "    W1 = 1e-4 * np.random.randn(hidden_size, num_classes)\n",
    "    W = [W0, W1]\n",
    "    \n",
    "  if b is None:\n",
    "    b0 = np.zeros((1, hidden_size))\n",
    "    b1 = np.zeros((1, num_classes))\n",
    "    b = [b0,b1]\n",
    "    \n",
    "  loss_history = []\n",
    "  train_acc_history = []\n",
    "  val_acc_history = []\n",
    "  \n",
    "  for it in range(num_iters):\n",
    "    X_batch = None\n",
    "    y_batch = None\n",
    "    \n",
    "    train_rows = np.arange(num_train)\n",
    "    idxs = np.random.choice(train_rows, batch_size, replace = False)  \n",
    "    \n",
    "    X_batch = X[idxs]\n",
    "    y_batch = y[idxs]\n",
    "    \n",
    "    layer1 = affine_forward(X_batch, W[0], b[0])\n",
    "    act1 = relu_forward(layer1)\n",
    "    \n",
    "    layer2 = affine_forward(act1, W[1], b[1])\n",
    "    \n",
    "    softmax_score = softmax(layer2)\n",
    "    loss, dout = softmax_loss(softmax_score, y_batch)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    dW1, db1, dact1 = affine_backward(dout, act1, W[1], b[1])\n",
    "    dlayer1 = relu_backward(dact1, act1)\n",
    "    \n",
    "    dW0, db0, dact0 = affine_backward(dlayer1, X_batch, W[0], b[1])\n",
    "    \n",
    "    W[0] = W[0]- dW0*learning_rate\n",
    "    b[0] = b[0] - db0*learning_rate\n",
    "    W[1] = W[1] - dW1*learning_rate\n",
    "    b[1] = b[1] - db1*learning_rate\n",
    "    \n",
    "    if verbose and it % 100 == 0:\n",
    "      print('Iterations', it, '/', num_iters, ':loss =', loss)\n",
    "      \n",
    "    if it % iterations_per_epoch == 0:\n",
    "      train_acc = (predict_two_layer_relu(X_batch, W, b) == y_batch).mean()\n",
    "      val_acc = (predict_two_layer_relu(X_val,W,b) == y_val).mean()\n",
    "      train_acc_history.append(train_acc)\n",
    "      val_acc_history.append(val_acc)\n",
    "      \n",
    "      learning_rate *= lr_decay\n",
    "  return loss_history, W, b, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaEpCP0yTU8W"
   },
   "source": [
    "# LETS DO IT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "id": "xoaLnMN7lKS1",
    "outputId": "4a054135-ffb1-406e-8518-fd97bdc44fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 0 / 2000 :loss = 2.3025510283228745\n",
      "Iterations 100 / 2000 :loss = 2.2954409103442788\n",
      "Iterations 200 / 2000 :loss = 2.248156100875269\n",
      "Iterations 300 / 2000 :loss = 2.1819157950704096\n",
      "Iterations 400 / 2000 :loss = 2.09111278605981\n",
      "Iterations 500 / 2000 :loss = 2.1005790572463847\n",
      "Iterations 600 / 2000 :loss = 2.0181231708890253\n",
      "Iterations 700 / 2000 :loss = 2.016968000548069\n",
      "Iterations 800 / 2000 :loss = 1.9855378206271115\n",
      "Iterations 900 / 2000 :loss = 2.107074726951833\n",
      "Iterations 1000 / 2000 :loss = 1.9686461501514492\n",
      "Iterations 1100 / 2000 :loss = 1.935286932017583\n",
      "Iterations 1200 / 2000 :loss = 1.9329819283571472\n",
      "Iterations 1300 / 2000 :loss = 1.9533240126923466\n",
      "Iterations 1400 / 2000 :loss = 1.9769873163982308\n",
      "Iterations 1500 / 2000 :loss = 1.8996830011646302\n",
      "Iterations 1600 / 2000 :loss = 1.8595131903580528\n",
      "Iterations 1700 / 2000 :loss = 1.9013100105509886\n",
      "Iterations 1800 / 2000 :loss = 1.8811238147778224\n",
      "Iterations 1900 / 2000 :loss = 1.8422126926849935\n",
      "Training Accuracy :  41.0 %\n",
      "Validation Accuracy :  33.6 %\n"
     ]
    }
   ],
   "source": [
    "loss, W_relu, b_relu, train_acc, val_acc = train_two_layer_relu(X_train, y_train, X_val, y_val, hidden_size = 1000, num_iters=2000,learning_rate=0.00007,)\n",
    "\n",
    "print('Training Accuracy : ', train_acc[-1]*100,'%')\n",
    "print('Validation Accuracy : ', val_acc[-1]*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9u-3T0zoz6O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Implement ANN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
